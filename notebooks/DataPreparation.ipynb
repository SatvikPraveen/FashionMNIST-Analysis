{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Weq6GfZSoAMA"
   },
   "outputs": [],
   "source": [
    "# Importing necessary modules\n",
    "import torch  # Core library for deep learning and tensor operations\n",
    "import torchvision  # Library for computer vision tasks and datasets\n",
    "from torchvision import datasets, transforms  # Importing datasets and transforms for data preparation\n",
    "import os  # Module for interacting with the file system\n",
    "from torch.utils.data import random_split #random_split`: Utility function from PyTorch for splitting datasets\n",
    "import pandas as pd  # Library for data manipulation and analysis; used here for saving datasets as CSV files\n",
    "# Setting up data transformations (e.g., converting images to tensors)\n",
    "transform = transforms.ToTensor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DvK_w-nUn1EC"
   },
   "source": [
    "## Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_AiEShVSlSRy",
    "outputId": "907ba4ca-a260-444a-8d64-797fd89106a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ../data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 26.4M/26.4M [00:48<00:00, 543kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/FashionMNIST/raw/train-images-idx3-ubyte.gz to ../data/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ../data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29.5k/29.5k [00:00<00:00, 246kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to ../data/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ../data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4.42M/4.42M [00:07<00:00, 615kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to ../data/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ../data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5.15k/5.15k [00:00<00:00, 9.41MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to ../data/FashionMNIST/raw\n",
      "\n",
      "\n",
      "âœ… Dataset loading and splitting completed successfully!\n",
      "ðŸ“‚ Data directory: ../data\n"
     ]
    }
   ],
   "source": [
    "# Define the root directory for storing datasets\n",
    "data_dir = \"../data\"\n",
    "\n",
    "# Check if the directory exists, if not create it\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "# Define transformations (converting images to tensors)\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "# Load the full training dataset\n",
    "full_train_data = datasets.FashionMNIST(\n",
    "    root=data_dir,\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "# Define the split ratio (e.g., 80% for training, 20% for validation)\n",
    "train_size = int(0.8 * len(full_train_data))\n",
    "validation_size = len(full_train_data) - train_size\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "train_data, validation_data = random_split(full_train_data, [train_size, validation_size])\n",
    "\n",
    "# Load the test dataset\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=data_dir,\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "print(\"\\nâœ… Dataset loading and splitting completed successfully!\")\n",
    "print(f\"ðŸ“‚ Data directory: {data_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cj7fRHDOlg3r",
    "outputId": "5b01d1c6-71f9-4b1c-96ef-cafae4a328c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŸ¢ Training set size: 48000 samples\n",
      "ðŸŸ¡ Validation set size: 12000 samples\n",
      "ðŸ”µ Test set size: 10000 samples\n"
     ]
    }
   ],
   "source": [
    "# Print dataset sizes for confirmation\n",
    "print(f\"ðŸŸ¢ Training set size: {len(train_data)} samples\")\n",
    "print(f\"ðŸŸ¡ Validation set size: {len(validation_data)} samples\")\n",
    "print(f\"ðŸ”µ Test set size: {len(test_data)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "OinFqDzemuEy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved train_data.csv to ../data_preparation\n",
      "âœ… Saved validation_data.csv to ../data_preparation\n",
      "âœ… Saved test_data.csv to ../data_preparation\n",
      "\n",
      "ðŸŽ‰ Data successfully saved to the data_preparation directory!\n"
     ]
    }
   ],
   "source": [
    "# Directory for saving CSV files\n",
    "csv_dir = \"../data_preparation\"\n",
    "\n",
    "# Check if the directories exist, if not create them\n",
    "os.makedirs(csv_dir, exist_ok=True)\n",
    "\n",
    "# Function to save a dataset as a CSV file\n",
    "def save_to_csv(dataset, filename):\n",
    "    data_list = []\n",
    "    for img, label in dataset:\n",
    "        # Flatten the image tensor and convert to a list\n",
    "        img_flat = img.view(-1).tolist()\n",
    "        data_list.append([label] + img_flat)\n",
    "\n",
    "    # Create a DataFrame\n",
    "    columns = ['label'] + [f'pixel_{i}' for i in range(len(img_flat))]\n",
    "    df = pd.DataFrame(data_list, columns=columns)\n",
    "\n",
    "    # Save the DataFrame as a CSV file in the data_preparation directory\n",
    "    csv_path = os.path.join(csv_dir, filename)\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    print(f\"âœ… Saved {filename} to {csv_dir}\")\n",
    "\n",
    "# Save the training, validation, and test datasets as CSV files\n",
    "save_to_csv(train_data, \"train_data.csv\")\n",
    "save_to_csv(validation_data, \"validation_data.csv\")\n",
    "save_to_csv(test_data, \"test_data.csv\")\n",
    "\n",
    "print(\"\\nðŸŽ‰ Data successfully saved to the data_preparation directory!\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (envf)",
   "language": "python",
   "name": "envf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
